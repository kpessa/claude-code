---
date: 2025-01-16T14:30:00
agent: database-researcher
type: research
topics: [database, patterns, performance, architecture]
tags: [#type/research, #topic/database, #domain/backend, #pattern/architecture, #performance/optimization]
related: [[Repository Pattern]], [[ACID Properties]], [[Caching Strategy]], [[Query Optimization]], [[Sharding Strategy]]
aliases: [Database Patterns, DB Patterns, Data Persistence Patterns]
status: current
---

# Database Patterns: Essential Patterns for Modern Web Applications

## üéØ Research Objective
Document essential database patterns, implementation strategies, and best practices for modern web applications. Focus on practical, reusable patterns that solve real-world problems developers face daily.

## üìã Executive Summary
Comprehensive analysis of 8 critical database patterns covering repository implementation, connection pooling, query optimization, caching strategies, migration patterns, NoSQL vs SQL decisions, transaction management, and sharding strategies. Each pattern includes problem/solution format with TypeScript/JavaScript examples and performance implications.
^summary

## üîç Key Findings

### Finding 1: Repository Pattern - Clean Data Access Layer

**Problem**: Direct database access scattered throughout application code leads to tight coupling, difficult testing, and duplicated query logic.

**Solution**: Implement Repository Pattern to encapsulate data access logic behind a collection-like interface.

```typescript
// Generic Repository Interface
interface Repository<T, ID> {
  findById(id: ID): Promise<T | null>;
  findAll(): Promise<T[]>;
  save(entity: T): Promise<T>;
  delete(id: ID): Promise<void>;
  findBy(criteria: Partial<T>): Promise<T[]>;
}

// User Repository Implementation
class UserRepository implements Repository<User, string> {
  constructor(private db: Database) {}
  
  async findById(id: string): Promise<User | null> {
    const result = await this.db.query(
      'SELECT * FROM users WHERE id = $1',
      [id]
    );
    return result.rows[0] ? this.mapToUser(result.rows[0]) : null;
  }
  
  async findAll(): Promise<User[]> {
    const result = await this.db.query('SELECT * FROM users');
    return result.rows.map(this.mapToUser);
  }
  
  async save(user: User): Promise<User> {
    if (user.id) {
      // Update existing
      await this.db.query(
        'UPDATE users SET name = $1, email = $2 WHERE id = $3',
        [user.name, user.email, user.id]
      );
    } else {
      // Create new
      const result = await this.db.query(
        'INSERT INTO users (name, email) VALUES ($1, $2) RETURNING id',
        [user.name, user.email]
      );
      user.id = result.rows[0].id;
    }
    return user;
  }
  
  async delete(id: string): Promise<void> {
    await this.db.query('DELETE FROM users WHERE id = $1', [id]);
  }
  
  async findBy(criteria: Partial<User>): Promise<User[]> {
    const { query, params } = this.buildWhereClause(criteria);
    const result = await this.db.query(
      `SELECT * FROM users ${query}`,
      params
    );
    return result.rows.map(this.mapToUser);
  }
  
  private mapToUser(row: any): User {
    return {
      id: row.id,
      name: row.name,
      email: row.email,
      createdAt: row.created_at
    };
  }
  
  private buildWhereClause(criteria: Partial<User>) {
    const conditions: string[] = [];
    const params: any[] = [];
    let paramIndex = 1;
    
    Object.entries(criteria).forEach(([key, value]) => {
      if (value !== undefined) {
        conditions.push(`${key} = $${paramIndex}`);
        params.push(value);
        paramIndex++;
      }
    });
    
    return {
      query: conditions.length > 0 ? `WHERE ${conditions.join(' AND ')}` : '',
      params
    };
  }
}

// Usage with Dependency Injection
class UserService {
  constructor(private userRepository: UserRepository) {}
  
  async createUser(userData: CreateUserDto): Promise<User> {
    const user = new User(userData);
    return await this.userRepository.save(user);
  }
  
  async getUserWithPosts(userId: string): Promise<UserWithPosts | null> {
    // Repository pattern allows for complex queries while maintaining abstraction
    const user = await this.userRepository.findById(userId);
    if (!user) return null;
    
    const posts = await this.postRepository.findBy({ userId });
    return { ...user, posts };
  }
}
```

**When to Use**: Medium to large applications with complex domain models, multiple data sources, or need for testable data access layer.

**When to Avoid**: Simple CRUD applications, prototypes, or when using sophisticated ORMs that already provide good abstraction.

**Performance Implications**: 
- Slightly higher memory usage due to abstraction layer
- Better query optimization opportunities through centralized query logic
- Easier to implement caching strategies

**Trade-offs**: 
- Additional complexity for simple operations
- Requires more upfront design
- Can lead to "fat" repositories if not properly decomposed

**Related Concepts**: [[Unit of Work]], [[Data Mapper]], [[Active Record]]

### Finding 2: Connection Pooling - Efficient Database Resource Management

**Problem**: Creating new database connections for each request causes performance bottlenecks and resource exhaustion.

**Solution**: Implement connection pooling to reuse database connections efficiently.

```typescript
// Connection Pool Configuration
interface PoolConfig {
  host: string;
  port: number;
  database: string;
  user: string;
  password: string;
  max: number;        // Maximum pool size
  min: number;        // Minimum pool size
  idleTimeoutMs: number;  // Close idle connections after this time
  connectionTimeoutMs: number;  // Timeout for getting connection from pool
  maxUses: number;    // Max uses per connection before replacement
}

// Pool Implementation with Monitoring
class DatabasePool {
  private pool: Pool;
  private metrics: PoolMetrics;
  
  constructor(config: PoolConfig) {
    this.pool = new Pool({
      ...config,
      // Performance optimizations
      max: config.max || 20,
      min: config.min || 5,
      idleTimeoutMillis: config.idleTimeoutMs || 30000,
      connectionTimeoutMillis: config.connectionTimeoutMs || 2000,
    });
    
    this.setupEventHandlers();
    this.metrics = new PoolMetrics();
  }
  
  private setupEventHandlers() {
    this.pool.on('connect', (client) => {
      this.metrics.incrementConnections();
      console.log('New client connected:', this.pool.totalCount);
    });
    
    this.pool.on('remove', (client) => {
      this.metrics.decrementConnections();
      console.log('Client removed:', this.pool.totalCount);
    });
    
    this.pool.on('error', (err, client) => {
      console.error('Pool error:', err);
      this.metrics.incrementErrors();
    });
  }
  
  // Single query - automatic connection handling
  async query<T = any>(text: string, params?: any[]): Promise<QueryResult<T>> {
    const start = Date.now();
    try {
      const result = await this.pool.query(text, params);
      this.metrics.recordQuery(Date.now() - start);
      return result;
    } catch (error) {
      this.metrics.incrementErrors();
      throw error;
    }
  }
  
  // Transaction support
  async transaction<T>(
    callback: (client: PoolClient) => Promise<T>
  ): Promise<T> {
    const client = await this.pool.connect();
    try {
      await client.query('BEGIN');
      const result = await callback(client);
      await client.query('COMMIT');
      return result;
    } catch (error) {
      await client.query('ROLLBACK');
      throw error;
    } finally {
      client.release();
    }
  }
  
  // Pool health monitoring
  getMetrics(): PoolStatus {
    return {
      totalConnections: this.pool.totalCount,
      idleConnections: this.pool.idleCount,
      waitingClients: this.pool.waitingCount,
      ...this.metrics.getStats()
    };
  }
  
  async close(): Promise<void> {
    await this.pool.end();
  }
}

// Usage patterns
class DatabaseService {
  constructor(private pool: DatabasePool) {}
  
  // Simple query
  async findUser(id: string): Promise<User | null> {
    const result = await this.pool.query(
      'SELECT * FROM users WHERE id = $1',
      [id]
    );
    return result.rows[0] || null;
  }
  
  // Transaction example
  async transferFunds(fromId: string, toId: string, amount: number): Promise<void> {
    await this.pool.transaction(async (client) => {
      // Debit from sender
      await client.query(
        'UPDATE accounts SET balance = balance - $1 WHERE user_id = $2',
        [amount, fromId]
      );
      
      // Credit to receiver
      await client.query(
        'UPDATE accounts SET balance = balance + $1 WHERE user_id = $2',
        [amount, toId]
      );
      
      // Log transaction
      await client.query(
        'INSERT INTO transactions (from_user, to_user, amount) VALUES ($1, $2, $3)',
        [fromId, toId, amount]
      );
    });
  }
}

// Health monitoring
class PoolMetrics {
  private queryCount = 0;
  private errorCount = 0;
  private totalQueryTime = 0;
  private connectionCount = 0;
  
  recordQuery(duration: number) {
    this.queryCount++;
    this.totalQueryTime += duration;
  }
  
  incrementErrors() { this.errorCount++; }
  incrementConnections() { this.connectionCount++; }
  decrementConnections() { this.connectionCount--; }
  
  getStats() {
    return {
      queryCount: this.queryCount,
      errorCount: this.errorCount,
      averageQueryTime: this.queryCount > 0 ? this.totalQueryTime / this.queryCount : 0,
      errorRate: this.queryCount > 0 ? this.errorCount / this.queryCount : 0
    };
  }
}
```

**When to Use**: All production applications, high-concurrency systems, applications with frequent database operations.

**Performance Implications**:
- 20-30ms saved per request by avoiding connection handshake
- Reduced memory usage on database server
- Better resource utilization under load

**Configuration Guidelines**:
- Max connections: 2-3x number of CPU cores
- Min connections: 25% of max connections
- Idle timeout: 30-60 seconds
- Connection timeout: 2-5 seconds

**Related Concepts**: [[Database Clustering]], [[Load Balancing]], [[Circuit Breaker]]

### Finding 3: Query Optimization - Preventing N+1 and Performance Issues

**Problem**: N+1 queries and inefficient query patterns cause severe performance degradation in web applications.

**Solution**: Implement eager loading, query batching, and strategic indexing patterns.

```typescript
// N+1 Problem Example and Solutions
class PostService {
  constructor(
    private postRepository: Repository<Post>,
    private userRepository: Repository<User>,
    private commentRepository: Repository<Comment>
  ) {}
  
  // ‚ùå BAD: N+1 Query Problem
  async getPostsWithAuthorsBad(): Promise<PostWithAuthor[]> {
    const posts = await this.postRepository.findAll(); // 1 query
    
    const postsWithAuthors = [];
    for (const post of posts) {
      // N queries (one for each post)
      const author = await this.userRepository.findById(post.authorId);
      postsWithAuthors.push({ ...post, author });
    }
    
    return postsWithAuthors; // Total: 1 + N queries
  }
  
  // ‚úÖ GOOD: Eager Loading Solution
  async getPostsWithAuthors(): Promise<PostWithAuthor[]> {
    const posts = await this.postRepository.findAll();
    const authorIds = [...new Set(posts.map(p => p.authorId))];
    
    // Single query to get all authors
    const authors = await this.userRepository.findByIds(authorIds);
    const authorMap = new Map(authors.map(a => [a.id, a]));
    
    return posts.map(post => ({
      ...post,
      author: authorMap.get(post.authorId)!
    })); // Total: 2 queries regardless of N
  }
  
  // ‚úÖ BETTER: Database-level JOIN
  async getPostsWithAuthorsOptimized(): Promise<PostWithAuthor[]> {
    const query = `
      SELECT 
        p.id, p.title, p.content, p.created_at,
        u.id as author_id, u.name as author_name, u.email as author_email
      FROM posts p
      INNER JOIN users u ON p.author_id = u.id
      ORDER BY p.created_at DESC
    `;
    
    const result = await this.db.query(query);
    return result.rows.map(this.mapToPostWithAuthor); // Total: 1 query
  }
  
  // Query batching for complex relationships
  async getPostsWithCommentsAndAuthors(): Promise<PostWithDetails[]> {
    const posts = await this.postRepository.findAll();
    const postIds = posts.map(p => p.id);
    const authorIds = [...new Set(posts.map(p => p.authorId))];
    
    // Batch queries
    const [comments, authors] = await Promise.all([
      this.commentRepository.findByPostIds(postIds),
      this.userRepository.findByIds(authorIds)
    ]);
    
    // Create lookup maps
    const commentsByPost = this.groupBy(comments, 'postId');
    const authorsById = new Map(authors.map(a => [a.id, a]));
    
    return posts.map(post => ({
      ...post,
      author: authorsById.get(post.authorId)!,
      comments: commentsByPost.get(post.id) || []
    })); // Total: 3 queries for any N
  }
  
  private groupBy<T, K extends keyof T>(array: T[], key: K): Map<T[K], T[]> {
    return array.reduce((map, item) => {
      const keyValue = item[key];
      if (!map.has(keyValue)) {
        map.set(keyValue, []);
      }
      map.get(keyValue)!.push(item);
      return map;
    }, new Map<T[K], T[]>());
  }
}

// Query Builder for Complex Scenarios
class QueryBuilder {
  private query = '';
  private params: any[] = [];
  private paramIndex = 1;
  
  select(fields: string): QueryBuilder {
    this.query += `SELECT ${fields} `;
    return this;
  }
  
  from(table: string): QueryBuilder {
    this.query += `FROM ${table} `;
    return this;
  }
  
  join(table: string, condition: string): QueryBuilder {
    this.query += `INNER JOIN ${table} ON ${condition} `;
    return this;
  }
  
  leftJoin(table: string, condition: string): QueryBuilder {
    this.query += `LEFT JOIN ${table} ON ${condition} `;
    return this;
  }
  
  where(condition: string, value?: any): QueryBuilder {
    if (value !== undefined) {
      condition = condition.replace('?', `$${this.paramIndex}`);
      this.params.push(value);
      this.paramIndex++;
    }
    this.query += `WHERE ${condition} `;
    return this;
  }
  
  orderBy(field: string, direction: 'ASC' | 'DESC' = 'ASC'): QueryBuilder {
    this.query += `ORDER BY ${field} ${direction} `;
    return this;
  }
  
  limit(count: number): QueryBuilder {
    this.query += `LIMIT ${count} `;
    return this;
  }
  
  build(): { query: string; params: any[] } {
    return { query: this.query.trim(), params: this.params };
  }
}

// Usage for complex queries
const complexQuery = new QueryBuilder()
  .select('p.*, u.name as author_name, COUNT(c.id) as comment_count')
  .from('posts p')
  .leftJoin('users u', 'p.author_id = u.id')
  .leftJoin('comments c', 'p.id = c.post_id')
  .where('p.published = ?', true)
  .orderBy('p.created_at', 'DESC')
  .limit(20)
  .build();

// Index strategy recommendations
const indexStrategies = {
  // Single column indexes for frequent WHERE clauses
  singleColumn: [
    'CREATE INDEX idx_posts_author_id ON posts(author_id)',
    'CREATE INDEX idx_posts_published ON posts(published)',
    'CREATE INDEX idx_comments_post_id ON comments(post_id)'
  ],
  
  // Composite indexes for multiple column queries
  composite: [
    'CREATE INDEX idx_posts_published_created ON posts(published, created_at DESC)',
    'CREATE INDEX idx_users_email_active ON users(email, active)'
  ],
  
  // Partial indexes for filtered queries
  partial: [
    'CREATE INDEX idx_posts_published_only ON posts(created_at DESC) WHERE published = true',
    'CREATE INDEX idx_users_active_only ON users(email) WHERE active = true'
  ]
};
```

**When to Use**: All database-driven applications, especially those with relational data.

**Performance Impact**: 
- N+1 elimination can reduce query count from hundreds to single digits
- Proper indexing can improve query performance by 10-100x
- Database-level JOINs are typically 2-5x faster than application-level joins

**Trade-offs**: 
- Eager loading uses more memory
- Complex queries can be harder to maintain
- Indexes speed up reads but slow down writes

**Related Concepts**: [[Database Indexing]], [[Query Planning]], [[ORM Optimization]]

### Finding 4: Caching Strategy - Multi-Layer Caching Pattern

**Problem**: Database queries become bottlenecks under high load, causing slow response times and resource exhaustion.

**Solution**: Implement strategic multi-layer caching with proper invalidation patterns.

```typescript
// Cache Interface for multiple implementations
interface Cache {
  get<T>(key: string): Promise<T | null>;
  set<T>(key: string, value: T, ttl?: number): Promise<void>;
  delete(key: string): Promise<void>;
  clear(): Promise<void>;
  has(key: string): Promise<boolean>;
}

// Redis Cache Implementation
class RedisCache implements Cache {
  constructor(private redis: Redis) {}
  
  async get<T>(key: string): Promise<T | null> {
    const value = await this.redis.get(key);
    return value ? JSON.parse(value) : null;
  }
  
  async set<T>(key: string, value: T, ttl = 3600): Promise<void> {
    await this.redis.setex(key, ttl, JSON.stringify(value));
  }
  
  async delete(key: string): Promise<void> {
    await this.redis.del(key);
  }
  
  async clear(): Promise<void> {
    await this.redis.flushdb();
  }
  
  async has(key: string): Promise<boolean> {
    const exists = await this.redis.exists(key);
    return exists === 1;
  }
}

// Memory Cache for L1 caching
class MemoryCache implements Cache {
  private cache = new Map<string, { value: any; expires: number }>();
  
  async get<T>(key: string): Promise<T | null> {
    const item = this.cache.get(key);
    if (!item) return null;
    
    if (Date.now() > item.expires) {
      this.cache.delete(key);
      return null;
    }
    
    return item.value;
  }
  
  async set<T>(key: string, value: T, ttl = 3600): Promise<void> {
    this.cache.set(key, {
      value,
      expires: Date.now() + (ttl * 1000)
    });
  }
  
  async delete(key: string): Promise<void> {
    this.cache.delete(key);
  }
  
  async clear(): Promise<void> {
    this.cache.clear();
  }
  
  async has(key: string): Promise<boolean> {
    const item = this.cache.get(key);
    return item !== undefined && Date.now() <= item.expires;
  }
}

// Multi-layer cache strategy
class CacheService {
  constructor(
    private l1Cache: MemoryCache,    // Fast in-memory cache
    private l2Cache: RedisCache,     // Distributed cache
    private database: DatabaseService
  ) {}
  
  async get<T>(key: string, fetcher: () => Promise<T>, ttl = 3600): Promise<T> {
    // L1 Cache check
    let value = await this.l1Cache.get<T>(key);
    if (value !== null) {
      return value;
    }
    
    // L2 Cache check
    value = await this.l2Cache.get<T>(key);
    if (value !== null) {
      // Store in L1 for faster subsequent access
      await this.l1Cache.set(key, value, Math.min(ttl, 300)); // L1 has shorter TTL
      return value;
    }
    
    // Database fetch
    value = await fetcher();
    
    // Store in both cache layers
    await Promise.all([
      this.l1Cache.set(key, value, Math.min(ttl, 300)),
      this.l2Cache.set(key, value, ttl)
    ]);
    
    return value;
  }
  
  async invalidate(pattern: string): Promise<void> {
    // Invalidate both cache layers
    if (pattern.includes('*')) {
      // Pattern-based invalidation
      await this.invalidatePattern(pattern);
    } else {
      // Single key invalidation
      await Promise.all([
        this.l1Cache.delete(pattern),
        this.l2Cache.delete(pattern)
      ]);
    }
  }
  
  private async invalidatePattern(pattern: string): Promise<void> {
    // Clear L1 completely for simplicity (could be optimized)
    await this.l1Cache.clear();
    
    // Use Redis pattern matching for L2
    const keys = await this.getKeysMatching(pattern);
    if (keys.length > 0) {
      await Promise.all(keys.map(key => this.l2Cache.delete(key)));
    }
  }
  
  private async getKeysMatching(pattern: string): Promise<string[]> {
    // Implementation depends on Redis client
    // This is a simplified version
    return []; // Would scan Redis for matching keys
  }
}

// Cache-Aside Pattern Implementation
class UserService {
  constructor(
    private userRepository: UserRepository,
    private cacheService: CacheService
  ) {}
  
  async getUser(id: string): Promise<User | null> {
    const cacheKey = `user:${id}`;
    
    return await this.cacheService.get(
      cacheKey,
      () => this.userRepository.findById(id),
      3600 // 1 hour TTL
    );
  }
  
  async updateUser(id: string, updates: Partial<User>): Promise<User> {
    // Update database
    const updatedUser = await this.userRepository.update(id, updates);
    
    // Invalidate related caches
    await Promise.all([
      this.cacheService.invalidate(`user:${id}`),
      this.cacheService.invalidate(`user:${id}:*`), // Related data
      this.cacheService.invalidate(`users:list:*`)  // List caches
    ]);
    
    return updatedUser;
  }
  
  async getUserPosts(userId: string): Promise<Post[]> {
    const cacheKey = `user:${userId}:posts`;
    
    return await this.cacheService.get(
      cacheKey,
      () => this.postRepository.findByUserId(userId),
      1800 // 30 minutes TTL (posts change more frequently)
    );
  }
}

// Write-Through Cache Pattern
class WriteThoughUserService {
  async updateUser(id: string, updates: Partial<User>): Promise<User> {
    // Update database first
    const updatedUser = await this.userRepository.update(id, updates);
    
    // Update cache immediately
    const cacheKey = `user:${id}`;
    await this.cacheService.l2Cache.set(cacheKey, updatedUser, 3600);
    await this.cacheService.l1Cache.set(cacheKey, updatedUser, 300);
    
    return updatedUser;
  }
}

// Cache warming strategy
class CacheWarmingService {
  constructor(
    private userService: UserService,
    private cacheService: CacheService
  ) {}
  
  // Warm cache with frequently accessed data
  async warmCache(): Promise<void> {
    const popularUsers = await this.userRepository.findPopularUsers(100);
    
    await Promise.all(
      popularUsers.map(user => 
        this.cacheService.l2Cache.set(`user:${user.id}`, user, 7200)
      )
    );
  }
  
  // Periodic cache refresh
  async scheduleRefresh(): Promise<void> {
    setInterval(async () => {
      await this.warmCache();
    }, 30 * 60 * 1000); // Every 30 minutes
  }
}
```

**When to Use**: High-traffic applications, read-heavy workloads, expensive computations, external API calls.

**Cache Strategy Guidelines**:
- **Cache-Aside**: Application manages cache (most common)
- **Write-Through**: Update cache immediately after database
- **Write-Behind**: Update cache immediately, database asynchronously
- **Refresh-Ahead**: Refresh cache before expiration

**TTL Recommendations**:
- User profiles: 1-4 hours
- Static content: 24 hours
- Frequently changing data: 5-30 minutes
- Session data: Match session timeout

**Related Concepts**: [[Cache Invalidation]], [[CDN Strategy]], [[Session Storage]]

### Finding 5: Database Migration Patterns - Versioned Schema Evolution

**Problem**: Database schema changes need to be applied consistently across environments without data loss or downtime.

**Solution**: Implement versioned migration system with rollback capabilities and zero-downtime strategies.

```typescript
// Migration Interface
interface Migration {
  version: string;
  description: string;
  up(): Promise<void>;
  down(): Promise<void>;
}

// Migration Runner
class MigrationRunner {
  constructor(private db: Database) {}
  
  async runMigrations(migrations: Migration[]): Promise<void> {
    await this.ensureMigrationTable();
    
    const appliedMigrations = await this.getAppliedMigrations();
    const pendingMigrations = migrations.filter(
      m => !appliedMigrations.includes(m.version)
    );
    
    for (const migration of pendingMigrations) {
      console.log(`Applying migration ${migration.version}: ${migration.description}`);
      
      await this.db.transaction(async (client) => {
        await migration.up();
        await this.recordMigration(migration.version, migration.description);
      });
      
      console.log(`‚úÖ Migration ${migration.version} applied successfully`);
    }
  }
  
  async rollback(targetVersion?: string): Promise<void> {
    const appliedMigrations = await this.getAppliedMigrations();
    const migrationsToRollback = targetVersion
      ? appliedMigrations.filter(v => v > targetVersion)
      : [appliedMigrations[appliedMigrations.length - 1]];
    
    for (const version of migrationsToRollback.reverse()) {
      const migration = this.findMigration(version);
      if (migration && migration.down) {
        console.log(`Rolling back migration ${version}`);
        await migration.down();
        await this.removeMigrationRecord(version);
      }
    }
  }
  
  private async ensureMigrationTable(): Promise<void> {
    await this.db.query(`
      CREATE TABLE IF NOT EXISTS schema_migrations (
        version VARCHAR(255) PRIMARY KEY,
        description TEXT,
        applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      )
    `);
  }
  
  private async getAppliedMigrations(): Promise<string[]> {
    const result = await this.db.query(
      'SELECT version FROM schema_migrations ORDER BY version'
    );
    return result.rows.map(row => row.version);
  }
  
  private async recordMigration(version: string, description: string): Promise<void> {
    await this.db.query(
      'INSERT INTO schema_migrations (version, description) VALUES ($1, $2)',
      [version, description]
    );
  }
}

// Example Migrations
const migration_001: Migration = {
  version: '001',
  description: 'Create users table',
  async up() {
    await db.query(`
      CREATE TABLE users (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        email VARCHAR(255) UNIQUE NOT NULL,
        name VARCHAR(255) NOT NULL,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      )
    `);
    
    await db.query(`
      CREATE INDEX idx_users_email ON users(email)
    `);
  },
  async down() {
    await db.query('DROP TABLE users');
  }
};

const migration_002: Migration = {
  version: '002',
  description: 'Add posts table',
  async up() {
    await db.query(`
      CREATE TABLE posts (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        title VARCHAR(500) NOT NULL,
        content TEXT,
        author_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
        published BOOLEAN DEFAULT false,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      )
    `);
    
    await db.query(`
      CREATE INDEX idx_posts_author ON posts(author_id)
    `);
    
    await db.query(`
      CREATE INDEX idx_posts_published ON posts(published, created_at DESC)
    `);
  },
  async down() {
    await db.query('DROP TABLE posts');
  }
};

// Zero-downtime migration pattern
const migration_003: Migration = {
  version: '003',
  description: 'Add email verification - zero downtime',
  async up() {
    // Step 1: Add new column as nullable
    await db.query(`
      ALTER TABLE users 
      ADD COLUMN email_verified BOOLEAN DEFAULT NULL
    `);
    
    // Step 2: Set default value for existing records
    await db.query(`
      UPDATE users 
      SET email_verified = false 
      WHERE email_verified IS NULL
    `);
    
    // Step 3: Add non-null constraint (after application deployment)
    // This would be in a separate migration after code deployment
  },
  async down() {
    await db.query('ALTER TABLE users DROP COLUMN email_verified');
  }
};

// Data migration pattern
const migration_004: Migration = {
  version: '004',
  description: 'Migrate user names to separate first/last name fields',
  async up() {
    // Add new columns
    await db.query(`
      ALTER TABLE users 
      ADD COLUMN first_name VARCHAR(255),
      ADD COLUMN last_name VARCHAR(255)
    `);
    
    // Migrate existing data
    const users = await db.query('SELECT id, name FROM users WHERE name IS NOT NULL');
    
    for (const user of users.rows) {
      const nameParts = user.name.split(' ');
      const firstName = nameParts[0] || '';
      const lastName = nameParts.slice(1).join(' ') || '';
      
      await db.query(`
        UPDATE users 
        SET first_name = $1, last_name = $2 
        WHERE id = $3
      `, [firstName, lastName, user.id]);
    }
    
    // Note: Don't drop old column yet - do that in a later migration
    // after ensuring application compatibility
  },
  async down() {
    await db.query(`
      ALTER TABLE users 
      DROP COLUMN first_name,
      DROP COLUMN last_name
    `);
  }
};

// Migration with seed data
const migration_005: Migration = {
  version: '005',
  description: 'Add admin user and default categories',
  async up() {
    // Create categories table
    await db.query(`
      CREATE TABLE categories (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        name VARCHAR(255) NOT NULL UNIQUE,
        slug VARCHAR(255) NOT NULL UNIQUE,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      )
    `);
    
    // Seed default categories
    const defaultCategories = [
      { name: 'Technology', slug: 'technology' },
      { name: 'Lifestyle', slug: 'lifestyle' },
      { name: 'Business', slug: 'business' }
    ];
    
    for (const category of defaultCategories) {
      await db.query(`
        INSERT INTO categories (name, slug) 
        VALUES ($1, $2) 
        ON CONFLICT (slug) DO NOTHING
      `, [category.name, category.slug]);
    }
    
    // Create admin user if not exists
    await db.query(`
      INSERT INTO users (email, first_name, last_name, email_verified)
      VALUES ('admin@example.com', 'Admin', 'User', true)
      ON CONFLICT (email) DO NOTHING
    `);
  },
  async down() {
    await db.query('DELETE FROM users WHERE email = $1', ['admin@example.com']);
    await db.query('DROP TABLE categories');
  }
};

// Environment-specific migration runner
class EnvironmentMigrationRunner extends MigrationRunner {
  constructor(
    db: Database,
    private environment: 'development' | 'staging' | 'production'
  ) {
    super(db);
  }
  
  async runMigrations(migrations: Migration[]): Promise<void> {
    // Production safety checks
    if (this.environment === 'production') {
      await this.performProductionChecks(migrations);
    }
    
    return super.runMigrations(migrations);
  }
  
  private async performProductionChecks(migrations: Migration[]): Promise<void> {
    // Check for destructive operations
    for (const migration of migrations) {
      const migrationCode = migration.up.toString();
      
      if (this.hasDestructiveOperations(migrationCode)) {
        throw new Error(
          `Migration ${migration.version} contains potentially destructive operations. ` +
          `Please review manually for production deployment.`
        );
      }
    }
    
    // Require explicit confirmation for production
    const confirmation = process.env.PRODUCTION_MIGRATION_CONFIRMED;
    if (confirmation !== 'true') {
      throw new Error(
        'Production migrations require PRODUCTION_MIGRATION_CONFIRMED=true environment variable'
      );
    }
  }
  
  private hasDestructiveOperations(code: string): boolean {
    const destructivePatterns = [
      /DROP\s+TABLE/i,
      /DROP\s+COLUMN/i,
      /TRUNCATE/i,
      /DELETE\s+FROM.*WHERE/i
    ];
    
    return destructivePatterns.some(pattern => pattern.test(code));
  }
}
```

**When to Use**: All applications with evolving database schemas, team development, production deployments.

**Best Practices**:
- Version migrations sequentially
- Make migrations idempotent
- Test rollbacks in staging
- Never modify existing migrations
- Use descriptive naming and comments

**Zero-Downtime Strategies**:
1. Add columns as nullable first
2. Deploy application code
3. Backfill data
4. Add constraints in separate migration
5. Remove old columns after verification

**Related Concepts**: [[Schema Versioning]], [[Blue-Green Deployment]], [[Database Rollback]]

### Finding 6: NoSQL vs SQL Decision Matrix

**Problem**: Choosing between SQL and NoSQL databases without clear decision criteria leads to architectural mismatches.

**Solution**: Use structured decision matrix based on specific application requirements and data characteristics.

```typescript
// Decision Matrix Interface
interface DatabaseDecisionFactors {
  dataStructure: 'highly-structured' | 'semi-structured' | 'unstructured';
  scalingPattern: 'vertical' | 'horizontal' | 'both';
  consistencyRequirements: 'strong' | 'eventual' | 'flexible';
  queryComplexity: 'simple' | 'moderate' | 'complex';
  transactionNeeds: 'acid-required' | 'basic' | 'none';
  teamExperience: 'sql-expert' | 'nosql-expert' | 'mixed' | 'beginner';
  projectPhase: 'early-prototype' | 'mvp' | 'scaling' | 'enterprise';
}

class DatabaseDecisionEngine {
  decide(factors: DatabaseDecisionFactors): DatabaseRecommendation {
    const score = this.calculateScore(factors);
    
    if (score.sql > score.nosql) {
      return {
        recommendation: 'SQL',
        confidence: score.sql / (score.sql + score.nosql),
        database: this.recommendSQLDatabase(factors),
        reasoning: this.generateSQLReasoning(factors)
      };
    } else {
      return {
        recommendation: 'NoSQL',
        confidence: score.nosql / (score.sql + score.nosql),
        database: this.recommendNoSQLDatabase(factors),
        reasoning: this.generateNoSQLReasoning(factors)
      };
    }
  }
  
  private calculateScore(factors: DatabaseDecisionFactors) {
    let sqlScore = 0;
    let nosqlScore = 0;
    
    // Data structure scoring
    switch (factors.dataStructure) {
      case 'highly-structured': sqlScore += 3; break;
      case 'semi-structured': sqlScore += 1; nosqlScore += 2; break;
      case 'unstructured': nosqlScore += 3; break;
    }
    
    // Scaling pattern scoring
    switch (factors.scalingPattern) {
      case 'vertical': sqlScore += 2; break;
      case 'horizontal': nosqlScore += 3; break;
      case 'both': sqlScore += 1; nosqlScore += 2; break;
    }
    
    // Consistency requirements
    switch (factors.consistencyRequirements) {
      case 'strong': sqlScore += 3; break;
      case 'eventual': nosqlScore += 2; break;
      case 'flexible': nosqlScore += 1; break;
    }
    
    // Query complexity
    switch (factors.queryComplexity) {
      case 'simple': nosqlScore += 2; break;
      case 'moderate': sqlScore += 1; nosqlScore += 1; break;
      case 'complex': sqlScore += 3; break;
    }
    
    // Transaction needs
    switch (factors.transactionNeeds) {
      case 'acid-required': sqlScore += 3; break;
      case 'basic': sqlScore += 1; break;
      case 'none': nosqlScore += 1; break;
    }
    
    // Team experience
    switch (factors.teamExperience) {
      case 'sql-expert': sqlScore += 2; break;
      case 'nosql-expert': nosqlScore += 2; break;
      case 'mixed': sqlScore += 1; nosqlScore += 1; break;
      case 'beginner': sqlScore += 1; break; // SQL generally easier to learn
    }
    
    return { sql: sqlScore, nosql: nosqlScore };
  }
  
  private recommendSQLDatabase(factors: DatabaseDecisionFactors): string {
    if (factors.projectPhase === 'early-prototype') return 'SQLite';
    if (factors.scalingPattern === 'vertical') return 'PostgreSQL';
    if (factors.queryComplexity === 'complex') return 'PostgreSQL';
    return 'PostgreSQL'; // Default recommendation
  }
  
  private recommendNoSQLDatabase(factors: DatabaseDecisionFactors): string {
    if (factors.dataStructure === 'unstructured') return 'MongoDB';
    if (factors.scalingPattern === 'horizontal') return 'MongoDB or Cassandra';
    if (factors.queryComplexity === 'simple') return 'Redis or DynamoDB';
    return 'MongoDB'; // Default recommendation
  }
}

// Usage examples with different scenarios
const ecommerceFactors: DatabaseDecisionFactors = {
  dataStructure: 'highly-structured', // Products, orders, customers
  scalingPattern: 'both',
  consistencyRequirements: 'strong', // Financial transactions
  queryComplexity: 'complex', // Analytics, reporting
  transactionNeeds: 'acid-required', // Payment processing
  teamExperience: 'sql-expert',
  projectPhase: 'scaling'
};

const contentManagementFactors: DatabaseDecisionFactors = {
  dataStructure: 'semi-structured', // Articles, media, user content
  scalingPattern: 'horizontal',
  consistencyRequirements: 'eventual',
  queryComplexity: 'moderate',
  transactionNeeds: 'basic',
  teamExperience: 'mixed',
  projectPhase: 'mvp'
};

const analyticsFactors: DatabaseDecisionFactors = {
  dataStructure: 'unstructured', // Event logs, user behavior
  scalingPattern: 'horizontal',
  consistencyRequirements: 'eventual',
  queryComplexity: 'simple', // Mostly aggregations
  transactionNeeds: 'none',
  teamExperience: 'nosql-expert',
  projectPhase: 'enterprise'
};

// Polyglot persistence pattern
class PolyglotPersistenceArchitecture {
  constructor(
    private userDB: PostgreSQLService,    // User profiles, auth
    private contentDB: MongoDBService,    // CMS content
    private cacheDB: RedisService,        // Session, cache
    private analyticsDB: ClickHouseService // Event tracking
  ) {}
  
  // Different data types go to appropriate stores
  async createUser(userData: CreateUserDto): Promise<User> {
    // Structured user data to PostgreSQL
    const user = await this.userDB.createUser(userData);
    
    // Cache user data in Redis
    await this.cacheDB.setUser(user.id, user);
    
    // Track user creation event
    await this.analyticsDB.trackEvent({
      type: 'user_created',
      userId: user.id,
      timestamp: new Date()
    });
    
    return user;
  }
  
  async createArticle(articleData: CreateArticleDto): Promise<Article> {
    // Flexible article content to MongoDB
    const article = await this.contentDB.createArticle(articleData);
    
    // Update user's article count in PostgreSQL
    await this.userDB.incrementUserArticleCount(article.authorId);
    
    // Cache popular articles
    if (article.featured) {
      await this.cacheDB.addToFeaturedArticles(article);
    }
    
    return article;
  }
}

// SQL vs NoSQL pattern examples
const sqlPatterns = {
  // Complex queries with JOINs
  complexReporting: `
    SELECT 
      u.name,
      COUNT(o.id) as order_count,
      SUM(oi.quantity * oi.price) as total_spent,
      AVG(oi.price) as avg_item_price
    FROM users u
    LEFT JOIN orders o ON u.id = o.user_id
    LEFT JOIN order_items oi ON o.id = oi.order_id
    WHERE o.created_at >= NOW() - INTERVAL '30 days'
    GROUP BY u.id, u.name
    HAVING total_spent > 100
    ORDER BY total_spent DESC
  `,
  
  // ACID transactions
  transferFunds: async (fromAccount: string, toAccount: string, amount: number) => {
    await db.transaction(async (client) => {
      await client.query('UPDATE accounts SET balance = balance - $1 WHERE id = $2', [amount, fromAccount]);
      await client.query('UPDATE accounts SET balance = balance + $1 WHERE id = $2', [amount, toAccount]);
      await client.query('INSERT INTO transactions (from_account, to_account, amount) VALUES ($1, $2, $3)', [fromAccount, toAccount, amount]);
    });
  }
};

const nosqlPatterns = {
  // Flexible document structure
  blogPost: {
    _id: ObjectId("..."),
    title: "Database Patterns",
    content: "...",
    author: {
      name: "John Doe",
      avatar: "https://..."
    },
    tags: ["database", "patterns", "web-dev"],
    comments: [
      {
        id: "comment1",
        author: "Jane Smith",
        content: "Great article!",
        timestamp: new Date(),
        likes: 5
      }
    ],
    metadata: {
      readTime: 8,
      difficulty: "intermediate",
      lastModified: new Date()
    }
  },
  
  // Horizontal scaling patterns
  shardedUserData: {
    // Shard by user ID
    getUserShard: (userId: string) => {
      const hash = createHash('md5').update(userId).digest('hex');
      const shardNumber = parseInt(hash.substr(0, 2), 16) % 4;
      return `users_shard_${shardNumber}`;
    }
  }
};
```

**Decision Guidelines**:

**Choose SQL When**:
- Strong consistency requirements
- Complex queries and reporting
- Well-defined, stable schema
- ACID transactions needed
- Team familiar with SQL

**Choose NoSQL When**:
- Rapid development and schema evolution
- Horizontal scaling requirements
- Document or key-value data model
- High write throughput
- Eventual consistency acceptable

**Hybrid Approach**: Use polyglot persistence for different data types within the same application.

**Related Concepts**: [[Polyglot Persistence]], [[CQRS]], [[Event Sourcing]]

### Finding 7: Transaction Patterns - ACID Compliance and Management

**Problem**: Managing data consistency and integrity across multiple operations without proper transaction patterns leads to data corruption and race conditions.

**Solution**: Implement comprehensive transaction patterns with proper isolation levels and error handling.

```typescript
// Transaction Manager with different isolation levels
class TransactionManager {
  constructor(private db: Database) {}
  
  async withTransaction<T>(
    operation: (client: TransactionClient) => Promise<T>,
    options: TransactionOptions = {}
  ): Promise<T> {
    const client = await this.db.connect();
    
    try {
      await this.beginTransaction(client, options);
      const result = await operation(client);
      await this.commitTransaction(client);
      return result;
    } catch (error) {
      await this.rollbackTransaction(client);
      throw error;
    } finally {
      client.release();
    }
  }
  
  private async beginTransaction(
    client: TransactionClient,
    options: TransactionOptions
  ): Promise<void> {
    await client.query('BEGIN');
    
    if (options.isolationLevel) {
      await client.query(
        `SET TRANSACTION ISOLATION LEVEL ${options.isolationLevel}`
      );
    }
    
    if (options.readOnly) {
      await client.query('SET TRANSACTION READ ONLY');
    }
  }
  
  private async commitTransaction(client: TransactionClient): Promise<void> {
    await client.query('COMMIT');
  }
  
  private async rollbackTransaction(client: TransactionClient): Promise<void> {
    await client.query('ROLLBACK');
  }
}

// Savepoint pattern for nested transactions
class SavepointManager {
  private savepointCounter = 0;
  
  async withSavepoint<T>(
    client: TransactionClient,
    operation: () => Promise<T>
  ): Promise<T> {
    const savepointName = `sp_${++this.savepointCounter}`;
    
    await client.query(`SAVEPOINT ${savepointName}`);
    
    try {
      const result = await operation();
      await client.query(`RELEASE SAVEPOINT ${savepointName}`);
      return result;
    } catch (error) {
      await client.query(`ROLLBACK TO SAVEPOINT ${savepointName}`);
      throw error;
    }
  }
}

// Banking transaction example with proper isolation
class BankingService {
  constructor(
    private transactionManager: TransactionManager,
    private savepointManager: SavepointManager
  ) {}
  
  async transferFunds(
    fromAccountId: string,
    toAccountId: string,
    amount: number,
    description?: string
  ): Promise<TransferResult> {
    return await this.transactionManager.withTransaction(
      async (client) => {
        // Check sufficient funds with SELECT FOR UPDATE
        const fromAccount = await client.query(
          'SELECT balance FROM accounts WHERE id = $1 FOR UPDATE',
          [fromAccountId]
        );
        
        if (!fromAccount.rows[0] || fromAccount.rows[0].balance < amount) {
          throw new Error('Insufficient funds');
        }
        
        // Use savepoint for the actual transfer operations
        return await this.savepointManager.withSavepoint(client, async () => {
          // Debit from source account
          await client.query(
            'UPDATE accounts SET balance = balance - $1, updated_at = NOW() WHERE id = $2',
            [amount, fromAccountId]
          );
          
          // Credit to destination account
          await client.query(
            'UPDATE accounts SET balance = balance + $1, updated_at = NOW() WHERE id = $2',
            [amount, toAccountId]
          );
          
          // Record transaction
          const transactionResult = await client.query(
            `INSERT INTO transactions (from_account_id, to_account_id, amount, description, created_at)
             VALUES ($1, $2, $3, $4, NOW())
             RETURNING id, created_at`,
            [fromAccountId, toAccountId, amount, description]
          );
          
          return {
            transactionId: transactionResult.rows[0].id,
            timestamp: transactionResult.rows[0].created_at,
            amount,
            fromAccountId,
            toAccountId
          };
        });
      },
      {
        isolationLevel: 'SERIALIZABLE', // Highest isolation for financial operations
        readOnly: false
      }
    );
  }
  
  // Batch processing with partial rollback
  async processBatchPayments(payments: PaymentRequest[]): Promise<BatchResult> {
    return await this.transactionManager.withTransaction(
      async (client) => {
        const results: PaymentResult[] = [];
        const errors: PaymentError[] = [];
        
        for (const payment of payments) {
          try {
            // Each payment in its own savepoint
            const result = await this.savepointManager.withSavepoint(
              client,
              () => this.processPayment(client, payment)
            );
            results.push(result);
          } catch (error) {
            // Savepoint rolled back, but transaction continues
            errors.push({
              paymentId: payment.id,
              error: error.message
            });
          }
        }
        
        return { successful: results, failed: errors };
      },
      { isolationLevel: 'READ_COMMITTED' }
    );
  }
  
  private async processPayment(
    client: TransactionClient,
    payment: PaymentRequest
  ): Promise<PaymentResult> {
    // Implement individual payment logic
    // This will be rolled back to savepoint if it fails
    // but won't affect other payments in the batch
    
    const result = await client.query(
      'INSERT INTO payments (account_id, amount, reference) VALUES ($1, $2, $3) RETURNING id',
      [payment.accountId, payment.amount, payment.reference]
    );
    
    await client.query(
      'UPDATE accounts SET balance = balance - $1 WHERE id = $2',
      [payment.amount, payment.accountId]
    );
    
    return {
      paymentId: result.rows[0].id,
      accountId: payment.accountId,
      amount: payment.amount,
      status: 'completed'
    };
  }
}

// Distributed transaction pattern (Two-Phase Commit simulation)
class DistributedTransactionManager {
  constructor(
    private databases: Database[],
    private transactionManager: TransactionManager
  ) {}
  
  async executeDistributedTransaction<T>(
    operations: ((db: Database) => Promise<T>)[]
  ): Promise<T[]> {
    const transactions: TransactionClient[] = [];
    
    try {
      // Phase 1: Prepare all transactions
      for (const db of this.databases) {
        const client = await db.connect();
        await client.query('BEGIN');
        transactions.push(client);
      }
      
      // Execute operations
      const results: T[] = [];
      for (let i = 0; i < operations.length; i++) {
        const result = await operations[i](this.databases[i]);
        results.push(result);
      }
      
      // Phase 2: Commit all transactions
      for (const client of transactions) {
        await client.query('COMMIT');
      }
      
      return results;
    } catch (error) {
      // Rollback all transactions
      for (const client of transactions) {
        try {
          await client.query('ROLLBACK');
        } catch (rollbackError) {
          console.error('Error during rollback:', rollbackError);
        }
      }
      throw error;
    } finally {
      // Release all connections
      for (const client of transactions) {
        client.release();
      }
    }
  }
}

// Optimistic locking pattern
class OptimisticLockingService {
  async updateWithOptimisticLock<T>(
    tableName: string,
    id: string,
    updates: Partial<T>,
    currentVersion: number
  ): Promise<T> {
    return await this.transactionManager.withTransaction(async (client) => {
      // Check current version
      const current = await client.query(
        `SELECT version FROM ${tableName} WHERE id = $1`,
        [id]
      );
      
      if (!current.rows[0]) {
        throw new Error('Record not found');
      }
      
      if (current.rows[0].version !== currentVersion) {
        throw new Error('Record was modified by another process');
      }
      
      // Update with version increment
      const updateFields = Object.keys(updates)
        .map((key, index) => `${key} = $${index + 3}`)
        .join(', ');
      
      const values = Object.values(updates);
      
      const result = await client.query(
        `UPDATE ${tableName} 
         SET ${updateFields}, version = version + 1, updated_at = NOW()
         WHERE id = $1 AND version = $2
         RETURNING *`,
        [id, currentVersion, ...values]
      );
      
      return result.rows[0];
    });
  }
}

// Event sourcing transaction pattern
class EventSourcingTransactionService {
  async appendEvents(
    streamId: string,
    events: DomainEvent[],
    expectedVersion: number
  ): Promise<void> {
    await this.transactionManager.withTransaction(async (client) => {
      // Check stream version
      const stream = await client.query(
        'SELECT version FROM event_streams WHERE stream_id = $1 FOR UPDATE',
        [streamId]
      );
      
      const currentVersion = stream.rows[0]?.version || 0;
      
      if (currentVersion !== expectedVersion) {
        throw new Error(`Concurrency conflict. Expected version ${expectedVersion}, current version ${currentVersion}`);
      }
      
      // Append events
      for (let i = 0; i < events.length; i++) {
        const event = events[i];
        const eventVersion = currentVersion + i + 1;
        
        await client.query(
          `INSERT INTO events (stream_id, event_type, event_data, version, created_at)
           VALUES ($1, $2, $3, $4, NOW())`,
          [streamId, event.type, JSON.stringify(event.data), eventVersion]
        );
      }
      
      // Update stream version
      const newVersion = currentVersion + events.length;
      
      if (stream.rows[0]) {
        await client.query(
          'UPDATE event_streams SET version = $1, updated_at = NOW() WHERE stream_id = $2',
          [newVersion, streamId]
        );
      } else {
        await client.query(
          'INSERT INTO event_streams (stream_id, version, created_at) VALUES ($1, $2, NOW())',
          [streamId, newVersion]
        );
      }
    }, { isolationLevel: 'SERIALIZABLE' });
  }
}
```

**Isolation Level Guidelines**:

- **READ_UNCOMMITTED**: Never use in production
- **READ_COMMITTED**: Default for most operations
- **REPEATABLE_READ**: For consistent reads within transaction
- **SERIALIZABLE**: For critical operations requiring full isolation

**Transaction Best Practices**:
- Keep transactions short and focused
- Acquire locks in consistent order to prevent deadlocks
- Use savepoints for complex operations
- Handle deadlock exceptions with retry logic
- Monitor transaction duration and lock contention

**Related Concepts**: [[Deadlock Prevention]], [[Lock Management]], [[Consistency Models]]

### Finding 8: Database Sharding - Horizontal Scaling Strategies

**Problem**: Single database instances cannot handle the scale requirements of high-growth applications.

**Solution**: Implement strategic database sharding with proper shard key selection and data distribution patterns.

```typescript
// Shard key strategies
type ShardKeyStrategy = 'hash' | 'range' | 'directory' | 'geographic';

interface ShardConfig {
  shardCount: number;
  strategy: ShardKeyStrategy;
  keyField: string;
  replicas?: number;
}

// Hash-based sharding
class HashShardingStrategy {
  constructor(private shardCount: number) {}
  
  getShardId(key: string): string {
    const hash = this.hash(key);
    const shardNumber = hash % this.shardCount;
    return `shard_${shardNumber}`;
  }
  
  private hash(key: string): number {
    let hash = 0;
    for (let i = 0; i < key.length; i++) {
      const char = key.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // Convert to 32-bit integer
    }
    return Math.abs(hash);
  }
  
  // Consistent hashing for easier resharding
  getConsistentShardId(key: string): string {
    const hash = this.consistentHash(key);
    const shardNumber = hash % this.shardCount;
    return `shard_${shardNumber}`;
  }
  
  private consistentHash(key: string): number {
    // Use a consistent hash function like MD5 or SHA-1
    const crypto = require('crypto');
    const hash = crypto.createHash('md5').update(key).digest('hex');
    return parseInt(hash.substr(0, 8), 16);
  }
}

// Range-based sharding
class RangeShardingStrategy {
  private ranges: Array<{ min: any; max: any; shardId: string }>;
  
  constructor(ranges: Array<{ min: any; max: any; shardId: string }>) {
    this.ranges = ranges.sort((a, b) => a.min - b.min);
  }
  
  getShardId(value: any): string {
    for (const range of this.ranges) {
      if (value >= range.min && value <= range.max) {
        return range.shardId;
      }
    }
    throw new Error(`No shard found for value: ${value}`);
  }
  
  // Dynamic range adjustment
  splitRange(shardId: string, splitPoint: any): void {
    const rangeIndex = this.ranges.findIndex(r => r.shardId === shardId);
    if (rangeIndex === -1) return;
    
    const range = this.ranges[rangeIndex];
    const newShardId = `${shardId}_split`;
    
    // Split the range
    this.ranges[rangeIndex] = { ...range, max: splitPoint };
    this.ranges.splice(rangeIndex + 1, 0, {
      min: splitPoint + 1,
      max: range.max,
      shardId: newShardId
    });
  }
}

// Directory-based sharding
class DirectoryShardingStrategy {
  private shardMap = new Map<string, string>();
  
  async getShardId(key: string): Promise<string> {
    // Check cache first
    if (this.shardMap.has(key)) {
      return this.shardMap.get(key)!;
    }
    
    // Lookup in directory service
    const shardId = await this.lookupInDirectory(key);
    this.shardMap.set(key, shardId);
    return shardId;
  }
  
  private async lookupInDirectory(key: string): Promise<string> {
    // Implementation would query a directory service
    // This is a simplified version
    return 'shard_0'; // Placeholder
  }
  
  async assignToShard(key: string, shardId: string): Promise<void> {
    this.shardMap.set(key, shardId);
    // Also update directory service
    await this.updateDirectory(key, shardId);
  }
  
  private async updateDirectory(key: string, shardId: string): Promise<void> {
    // Update external directory service
  }
}

// Shard manager coordinating multiple strategies
class ShardManager {
  private shards = new Map<string, Database>();
  private strategy: ShardKeyStrategy;
  
  constructor(
    private config: ShardConfig,
    private databases: Map<string, Database>
  ) {
    this.shards = databases;
    this.initializeStrategy();
  }
  
  private initializeStrategy(): void {
    switch (this.config.strategy) {
      case 'hash':
        this.strategy = new HashShardingStrategy(this.config.shardCount);
        break;
      case 'range':
        // Initialize with ranges
        this.strategy = new RangeShardingStrategy([
          { min: 0, max: 999, shardId: 'shard_0' },
          { min: 1000, max: 1999, shardId: 'shard_1' },
          // ... more ranges
        ]);
        break;
      case 'directory':
        this.strategy = new DirectoryShardingStrategy();
        break;
      default:
        throw new Error(`Unsupported sharding strategy: ${this.config.strategy}`);
    }
  }
  
  async query<T>(
    shardKey: string,
    sql: string,
    params: any[] = []
  ): Promise<T[]> {
    const shardId = await this.getShardId(shardKey);
    const database = this.shards.get(shardId);
    
    if (!database) {
      throw new Error(`Shard not found: ${shardId}`);
    }
    
    const result = await database.query(sql, params);
    return result.rows;
  }
  
  async queryAllShards<T>(
    sql: string,
    params: any[] = []
  ): Promise<Map<string, T[]>> {
    const results = new Map<string, T[]>();
    
    await Promise.all(
      Array.from(this.shards.entries()).map(async ([shardId, database]) => {
        try {
          const result = await database.query(sql, params);
          results.set(shardId, result.rows);
        } catch (error) {
          console.error(`Error querying shard ${shardId}:`, error);
          results.set(shardId, []);
        }
      })
    );
    
    return results;
  }
  
  private async getShardId(shardKey: string): Promise<string> {
    if (this.strategy instanceof DirectoryShardingStrategy) {
      return await this.strategy.getShardId(shardKey);
    } else {
      return (this.strategy as any).getShardId(shardKey);
    }
  }
  
  // Cross-shard transaction (use sparingly)
  async crossShardTransaction<T>(
    operations: Array<{
      shardKey: string;
      operation: (db: Database) => Promise<T>;
    }>
  ): Promise<T[]> {
    const transactions = new Map<string, Database>();
    
    try {
      // Begin transactions on all involved shards
      for (const op of operations) {
        const shardId = await this.getShardId(op.shardKey);
        const database = this.shards.get(shardId);
        if (!database) throw new Error(`Shard not found: ${shardId}`);
        
        if (!transactions.has(shardId)) {
          await database.query('BEGIN');
          transactions.set(shardId, database);
        }
      }
      
      // Execute operations
      const results: T[] = [];
      for (const op of operations) {
        const shardId = await this.getShardId(op.shardKey);
        const database = this.shards.get(shardId)!;
        const result = await op.operation(database);
        results.push(result);
      }
      
      // Commit all transactions
      for (const database of transactions.values()) {
        await database.query('COMMIT');
      }
      
      return results;
    } catch (error) {
      // Rollback all transactions
      for (const database of transactions.values()) {
        try {
          await database.query('ROLLBACK');
        } catch (rollbackError) {
          console.error('Rollback error:', rollbackError);
        }
      }
      throw error;
    }
  }
}

// Application service using sharded data
class ShardedUserService {
  constructor(private shardManager: ShardManager) {}
  
  async createUser(userData: CreateUserDto): Promise<User> {
    const userId = generateUserId();
    const shardKey = userId; // Use user ID as shard key
    
    const result = await this.shardManager.query(
      shardKey,
      'INSERT INTO users (id, email, name) VALUES ($1, $2, $3) RETURNING *',
      [userId, userData.email, userData.name]
    );
    
    return result[0];
  }
  
  async getUser(userId: string): Promise<User | null> {
    const results = await this.shardManager.query<User>(
      userId,
      'SELECT * FROM users WHERE id = $1',
      [userId]
    );
    
    return results[0] || null;
  }
  
  async getAllUsers(): Promise<User[]> {
    const allResults = await this.shardManager.queryAllShards<User>(
      'SELECT * FROM users ORDER BY created_at DESC'
    );
    
    // Merge and sort results from all shards
    const users: User[] = [];
    for (const shardResults of allResults.values()) {
      users.push(...shardResults);
    }
    
    return users.sort((a, b) => 
      new Date(b.createdAt).getTime() - new Date(a.createdAt).getTime()
    );
  }
  
  // Complex cross-shard operation
  async getUsersWithPosts(userIds: string[]): Promise<UserWithPosts[]> {
    const usersByShards = new Map<string, string[]>();
    
    // Group users by shard
    for (const userId of userIds) {
      const shardId = await this.shardManager.getShardId(userId);
      if (!usersByShards.has(shardId)) {
        usersByShards.set(shardId, []);
      }
      usersByShards.get(shardId)!.push(userId);
    }
    
    // Query each shard
    const promises = Array.from(usersByShards.entries()).map(
      async ([shardId, shardUserIds]) => {
        const placeholders = shardUserIds.map((_, i) => `$${i + 1}`).join(',');
        
        return await this.shardManager.query<UserWithPosts>(
          shardUserIds[0], // Use first user ID to identify shard
          `SELECT 
             u.*, 
             COALESCE(json_agg(p.*) FILTER (WHERE p.id IS NOT NULL), '[]') as posts
           FROM users u
           LEFT JOIN posts p ON u.id = p.author_id
           WHERE u.id IN (${placeholders})
           GROUP BY u.id`,
          shardUserIds
        );
      }
    );
    
    const results = await Promise.all(promises);
    return results.flat();
  }
}

// Resharding strategy for scaling
class ReshardingManager {
  constructor(private shardManager: ShardManager) {}
  
  async reshardData(
    sourceShardId: string,
    targetShards: string[],
    batchSize = 1000
  ): Promise<void> {
    const sourceDB = this.shardManager.shards.get(sourceShardId);
    if (!sourceDB) throw new Error(`Source shard not found: ${sourceShardId}`);
    
    let offset = 0;
    let hasMore = true;
    
    while (hasMore) {
      // Fetch batch from source
      const batch = await sourceDB.query(
        `SELECT * FROM users LIMIT $1 OFFSET $2`,
        [batchSize, offset]
      );
      
      if (batch.rows.length === 0) {
        hasMore = false;
        continue;
      }
      
      // Distribute batch to target shards
      for (const row of batch.rows) {
        const newShardId = this.calculateNewShardId(row.id, targetShards);
        const targetDB = this.shardManager.shards.get(newShardId);
        
        if (targetDB) {
          await targetDB.query(
            'INSERT INTO users (id, email, name) VALUES ($1, $2, $3) ON CONFLICT DO NOTHING',
            [row.id, row.email, row.name]
          );
        }
      }
      
      offset += batchSize;
    }
  }
  
  private calculateNewShardId(key: string, targetShards: string[]): string {
    const hash = this.hash(key);
    const index = hash % targetShards.length;
    return targetShards[index];
  }
  
  private hash(key: string): number {
    // Same hash function as used in sharding strategy
    let hash = 0;
    for (let i = 0; i < key.length; i++) {
      const char = key.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash;
    }
    return Math.abs(hash);
  }
}
```

**Sharding Strategy Selection**:

- **Hash Sharding**: Even distribution, difficult to range queries
- **Range Sharding**: Good for range queries, can create hotspots
- **Directory Sharding**: Flexible but adds lookup overhead
- **Geographic Sharding**: Latency optimization, regulatory compliance

**When to Shard**:
- Database size > 100GB-1TB
- Query performance degrading despite optimization
- Write throughput exceeding single instance capacity
- Need for geographic distribution

**Challenges**:
- Cross-shard queries become complex
- Transactions across shards are expensive
- Resharding is operationally complex
- Application complexity increases significantly

**Related Concepts**: [[Consistent Hashing]], [[Data Partitioning]], [[Distributed Systems]]

## üìä Analysis

### Current State
Modern web applications face increasing data scale and complexity challenges that require sophisticated database patterns. Traditional single-database approaches often become bottlenecks as applications grow.

### Opportunities
- Repository Pattern provides clean abstraction for testing and maintenance
- Connection pooling offers immediate performance improvements
- Query optimization can dramatically reduce response times
- Multi-layer caching enables horizontal scaling
- Proper migration patterns ensure zero-downtime deployments
- Strategic database selection prevents architectural lock-in
- Transaction patterns ensure data integrity
- Sharding enables unlimited horizontal scaling

### Challenges
- Pattern complexity can over-engineer simple applications
- Distributed systems introduce consistency challenges
- Cross-pattern integration requires careful design
- Performance optimization needs continuous monitoring
- Migration strategies require coordination across teams

### Recommendations

1. **Immediate**: 
   - Implement connection pooling for all database connections
   - Add query optimization patterns to prevent N+1 problems
   - Establish migration versioning system

2. **Short-term**: 
   - Implement repository pattern for testable data access
   - Add multi-layer caching for read-heavy operations
   - Create transaction patterns for critical operations

3. **Long-term**: 
   - Evaluate NoSQL vs SQL decisions for different data types
   - Consider sharding strategies for horizontal scaling
   - Implement distributed transaction patterns where needed

## üîó Connections

### Relates To
- [[Performance Optimization]] - Database patterns directly impact application performance
- [[Testing Patterns]] - Repository pattern enables better unit testing
- [[Security Patterns]] - Transaction patterns prevent data corruption attacks

### Enables
- [[Microservices Architecture]] - Database per service pattern
- [[Event Sourcing]] - Transaction patterns support event-driven architectures
- [[CQRS Implementation]] - Read/write separation with appropriate database choices

### Conflicts With
- [[Rapid Prototyping]] - Complex patterns can slow initial development
- [[Simple CRUD Applications]] - Over-engineering for basic requirements

## üí° Insights

Cross-cutting observations reveal that successful database architectures often combine multiple patterns strategically rather than using single solutions. The polyglot persistence approach allows optimal database selection per use case while connection pooling and caching provide universal performance benefits.

Most applications benefit from starting simple with repository patterns and connection pooling, then adding complexity (sharding, distributed transactions) only when scale demands it.

## üìà Metrics/Data

| Pattern | Implementation Effort | Performance Impact | Scalability Benefit | Complexity |
|---------|----------------------|-------------------|-------------------|------------|
| Repository | Medium | Low | Medium | Low |
| Connection Pooling | Low | High | High | Low |
| Query Optimization | Medium | Very High | High | Medium |
| Caching | Medium | Very High | Very High | Medium |
| Migrations | Low | Low | Medium | Low |
| NoSQL Decision | High | Variable | High | High |
| Transactions | Medium | Medium | Medium | Medium |
| Sharding | Very High | Medium | Very High | Very High |

## üéØ Action Items

- [ ] Implement connection pooling across all database connections
- [ ] Create repository pattern templates for common entities
- [ ] Set up query performance monitoring and N+1 detection
- [ ] Design multi-layer caching strategy with Redis
- [ ] Establish database migration workflow and versioning
- [ ] Create decision matrix tool for NoSQL vs SQL choices
- [ ] Implement transaction patterns for critical business operations
- [ ] Evaluate sharding requirements for high-growth scenarios

## üìö Sources

- [Repository Pattern - Martin Fowler](https://martinfowler.com/eaaCatalog/repository.html)
- [Oracle Database ACID Properties](https://docs.oracle.com/en/database/oracle/oracle-database/19/cncpt/data-concurrency-and-consistency.html)
- [Node.js PostgreSQL Pooling](https://node-postgres.com/features/pooling)
- [Rails Active Record Query Interface](https://guides.rubyonrails.org/active_record_querying.html)
- [PostgreSQL Performance Tips](https://www.postgresql.org/docs/current/performance-tips.html)
- [AWS Database Sharding Guide](https://aws.amazon.com/what-is/database-sharding/)
- Industry best practices and patterns analysis

## üè∑Ô∏è Tags

#type/research #topic/database #domain/backend #pattern/architecture #performance/optimization #status/current

## üìù Notes

This research covers essential database patterns but could be extended with:
- Event sourcing implementation patterns
- CQRS with database separation
- Time-series database patterns
- Graph database patterns for relationship-heavy data
- Database testing strategies and patterns
- Backup and disaster recovery patterns

Future research should focus on emerging patterns like database-as-code and infrastructure-driven database management.

---
*Research conducted by database-researcher on 2025-01-16*